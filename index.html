---
layout: home
---
<figure>
    <img src="/assets/img/rlfm_logo.png">
</figure>

<center>
    <h1>Reinforcement Learning and Foundation Models</h1>
</center>

<br>
<div class="subsubheading">ICLR 2024 hybrid Workshop, May 7th, Vienna, Austria</div>

<div class="contact-heading"><a href='mailto:rlfmworkshop@gmail.com'>rlfmworkshop@gmail.com</a>.</div>

<hr class="small" style="border-width: 1pt; border-color: lightgray;">


<!--<h3 style='margin-bottom: 10pt;'>Sponsors</h3>-->

<!--<div class="center">-->
<!--	<div class='description' style='font-size: 11pt;'>-->
<!--This workshop is sponsored by:-->
<!--</div>-->
<!--  <div class="logo-sponsor"><img src="assets/img/sponsors/deepmind_logo.jpg" style="width:100pt;"></div>-->
<!--  <div class style="font-size: 11pt;">and</div>-->
<!--  <div class="logo-sponsor"><img src="assets/img/sponsors/cohere_wordmark_black.png" style="width:55pt; margin-bottom: 6pt;"></div>-->
<!--</div>-->

<div class='description' style='font-size: 11pt;'>

    <h3 style='margin-bottom: 10pt;'>Description</h3>
    <p>
        Reinforcement learning (RL) has long been proposed as a powerful paradigm for learning, from interaction, to solve sequential decision-making problems. Very recently, there has been a surge of work using RL to align large language models (LLMs) to human values through the paradigm of RL from human feedback (RLHF). Inspired by the idea of alignment through inverse reinforcement learning, these approaches use human preference data to learn a reward function approximating human values and use this reward to finetune LLMs with RL. RLHF has taken a step in helping solve the outstanding problems with language models by making them more similar to a conversational partner, more steerable, more relevant, and more aligned. Additionally, one can also see it the other way around: viewing instruction-following assistants as RL agents. These language-based models solve some of the problems of pure-RL agents such as the ability to flexibly multitask and use pretrained experience instead of being trained from scratch.
    </p>
    <p>
        The aim of this workshop is to provide a stage for researchers in this exciting field to define common challenges and questions, to exchange and consolidate best practices, and to push the advance of tools and benchmarks. As an indication, this workshop aims to make progress on the following open questions:
        <ul>
        <li>  What are the advantages of fine-tuning LLMs with RL compared to behavioral cloning/supervised learning, and on what tasks? </li>
        <li>  What are the recommended methods for training reward functions using human feedback? In what ways can various forms of feedback, such as preferences, demonstrations, and advice, be utilized? How can the quality of human feedback be assessed? </li>
        <li>  What approaches should be considered when training RLLM agents using pretrained LMs? This includes considerations for hyperparameters, reproducibility, algorithms, generation strategies, and the use of parameter-efficient fine-tuning. </li>
        <li>  How can RL be utilized to develop conversational AI that is capable of generating truthful responses? </li>
        <li>  Can RLLM agents effectively handle tasks in formal domains that involve automated checks for completion, such as mathematics (using a proof system) or programming (interacting with a compiler or interpreter)? </li>
        <li>  In what ways can game-playing agent techniques, such as self-play and multi-agent dynamics, be utilized to enhance the performance of RLLM agents? </li>
        </ul>

        The field is emerging, and we expect to formalize many other questions as the workshop takes place. The workshop will additionally aim at fostering discussions around the ethical implications of RLLM research (whose preferences are we optimizing?), as well as serve as a platform for the presentation of tools and frameworks that make training RLLM agents easier and more accessible to a wider community.
        
        
    </p>
</div>

<h3 style='margin-bottom: 10pt;'>References</h3>
<div class='references' style='font-size:9pt'>
    <ul>
        <li>Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
            Beyond tabula rasa: Reincarnating reinforcement learning. arXiv preprint arXiv:2206.01626, 2022. </li>

    </ul>
</div>
